\documentclass[sigconf]{acmart}
\settopmatter{printacmref=false}
% ---------- Required packages (ACM whitelist) ----------
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{hyperref}

% ---------- Metadata ----------
\title{Enhancing Multi-Task User Representations through Residual-Contrastive Autoencoders}

\author{Mluleki Wenkaba Mtande}
\affiliation{
  \institution{Independent Researcher}
  \city{Taichung}
  \country{Taiwan}
}
\email{mlumtande@gmail.com}

\begin{abstract}
The RecSys Challenge 2025 addresses the growing need for unified user modeling by tasking participants to develop universal user representations from large-scale behavioral data. These representations must effectively capture patterns across millions of purchase, cart, and browsing events while generalizing to multiple predictive tasks, both disclosed and hidden. This paper presents my approach: a novel autoencoder framework that combines residual connections, contrastive regularization, and curriculum noise to create robust, transferable user embeddings. I demonstrate how this architecture, when paired with comprehensive feature engineering, successfully generates universal representations that maintain high performance across diverse prediction tasks. The experimental results reveal significant improvements over traditional methods, highlighting the effectiveness of this integrated approach for developing generalizable, robust, and application-agnostic user models. Code is available at \url{https://github.com/mlu1/Deep-learning-projects/tree/master/project17}
\end{abstract}

\ccsdesc[500]{Information systems~Recommender systems}
\ccsdesc[300]{Computing methodologies~Unsupervised learning}
\ccsdesc[300]{Computing methodologies~Neural networks}

\keywords{Recommender Systems, Representation Learning, User Embeddings, Autoencoders, Contrastive Learning, Residual Networks, Multi-task Learning, Behavioral Modeling}
\acmConference[RecSys '25]{Proceedings of the
19th ACM Conference on Recommender Systems}{October 5--9, 2025}{Shanghai, China}
\begin{document}

\maketitle

\section{Introduction}
Predictive analytics serves as the backbone of modern digital applications, powering everything from personalized recommendations, churn prediction, and propensity estimation. Traditionally, these tasks are approached independently, despite all relying on the same core resource which is the detailed logs of user behavior. This fragmentation not only creates redundant modeling efforts but also significantly limits the generalizability of the resulting systems.

The RecSys Challenge 2025 \cite{recsys2025} directly addresses this issue head on by promoting a unified approach to behavior modeling.  The challenge is organized through a collaboration between Synerise and an international team of academic experts, the challenge introduces the concept of \emph{Universal Behavioral Profiles}: compact user representations meant to capture essential patterns across diverse interactions purchases, cart actions, page views, and search queries. The central objective is to produce embeddings that perform robustly across multiple downstream tasks, both disclosed and undisclosed, reflecting their generalization power.

Participants are provided with a massive, real world dataset containing millions of multi type user events. Rather than optimizing for a single target, submitted embeddings are evaluated in several predictive tasks including churn and product propensity prediction using a composite score that aggregates performance across all tasks. This setup not only emphasizes robust, transferable representation learning, but also mirrors real world business requirements, where unified user profiles support a range of analytics and decision making systems.


\subsection{Problem Statement}
The core challenge of the RecSys Challenge 2025 lies in developing \emph{Universal Behavioral Profiles}. The obtained user representations that effectively capture the essence of diverse interactions including purchases, cart modifications, searches, and page visits on online shopping platforms. Unlike conventional approaches that develop separate models for individual predictive tasks, this challenge demands a more elegant solution which  a single, compact embedding per user that generalizes effectively across various downstream applications.

The dataset comprises millions of user events, creating a complex landscape where the goal is to construct embeddings that excel not just on known predictive tasks like churn and product propensity, but also on undisclosed tasks determined by the organizers. The quality assessment uses a composite score that aggregates performance across all evaluation tasks, pushing participants to develop truly universal representations.

This approach introduces significant technical hurdles, requiring representations that simultaneously capture broad behavioral patterns and nuanced individual preferences. Participants must navigate challenges of scale, data heterogeneity, and information preservation to create embeddings that remain relevant across diverse business objectives. The ideal solution balances compression efficiency with the retention of task critical information, enabling high performance across both anticipated and unexpected prediction scenarios.
\subsection{Dataset}
The RecSys Challenge 2025 provides participants with an extensive dataset containing over 168 million events generated by approximately 19 million users on a real-world e-commerce platform. Each event carries a timestamp and falls into one of several key categories: product purchases, cart additions and removals, page visits, and search queries. This dataset presents researchers with several distinctive challenges that make effective representation learning both crucial and complex.

The data exhibits high cardinality in both user and item dimensions, creating a sparse interaction matrix that complicates traditional collaborative filtering approaches. User behavior shows significant variation in interaction frequency, with some users generating hundreds of events while others appear only sporadically. The temporal dimension adds another layer of complexity, with irregular intervals between events and evolving user preferences over time. These characteristics of high cardinality, varying interaction frequencies, and temporal sparsity create a rich but challenging environment where sophisticated representation learning techniques become essential for capturing meaningful patterns that generalize across different predictive tasks.

\label{sec:related}
\section{Related Work}

Autoencoders have become a foundational approach for unsupervised representation learning in domains such as recommender systems, user modeling, and behavioral analytics~\cite{Hinton2006, Vincent2008, Kingma2014, Sedhain2015}. By compressing high dimensional input data into lower dimensional latent representations, autoencoders can uncover underlying structure and facilitate a wide range of downstream predictive tasks. Numerous architectural enhancements have been proposed, including denoising objectives~\cite{Vincent2008}, variational formulations~\cite{Kingma2014}, and residual connections~\cite{He2016ResNet, Mao2016}. These innovations address issues such as information loss, vanishing gradients, and improved robustness to noise.

Beyond autoencoders, the field of representation learning has been shaped by contrastive learning~\cite{Oord2018, Chen2020SimCLR}, self-supervised objectives, and metric learning, all of which aim to produce embeddings that capture meaningful similarities and differences in the data. In recent recommender systems research, combining autoencoder architectures with contrastive or self-supervised regularization has shown improved generalization and task transfer~\cite{Zhou2020S3Rec, Zhan2022CLAES}. However, learning representations that remain robust across heterogeneous event types and diverse predictive tasks, as required in the RecSys Challenge 2025, remains an open research problem.

\section{Proposed Method}
\label{sec:method}

\subsection{Feature Engineering Pipeline}

To maximize the representational power of the universal user embeddings, I design a comprehensive feature engineering pipeline that integrates best practices and recent advances in user behavior modeling~\cite{Quadrana2018, Christoffel2022, Covington2016, Rendle2010, Hidasi2016, Wang2019}.

\textbf{Temporal Features.}
Cyclical time patterns are encoded via sine and cosine transformations of hour-of-day and day-of-week~\cite{Laptev2017}, allowing the model to recognize periodic routines and seasonality. User activity momentum is captured by aggregating session counts, activity deltas, and moving averages over sliding temporal windows~\cite{Quadrana2018}. These features have been shown to improve personalization and capture recency effects in recommender systems.

\textbf{Session Features.}
Sessionization is performed using a 30 minute inactivity threshold, a standard approach in clickstream analysis~\cite{Yang2020, Quadrana2017}. For each user, I compute statistics on session duration (mean, variance), inter-session time gaps, burstiness (ratio of high activity to low activity sessions), and part-of-day distributions~\cite{Christoffel2022}. Such features capture engagement intensity, temporal shopping intent, and periodicity in usage.

\textbf{Behavioral Diversity and Exploration.}
To measure behavioral diversity, I employ entropy and Gini coefficients over item categories and brands~\cite{Rendle2010}. Exploration rates are quantified as the ratio of unique SKUs or categories viewed to total actions—reflect users’ openness to discovering new content~\cite{Cui2018}. Novelty seeking behavior is explicitly modeled using time weighted counts of new-item interactions~\cite{Zhao2019}.

\textbf{Conversion and Funnel Metrics.}
By analyzing  previous implementations of solutions that aim to help digital commerce ~\cite{Covington2016, Zhou2020S3Rec}, I calculate cart abandonment rates, session level conversion efficiency (search-to-purchase ratios), and immediate (same-session) conversions. Price sensitivity is measured by binning products into price segments and correlating with purchase probability, inspired by methods in customer value modeling~\cite{Krasnova2018}.

\textbf{Advanced Behavioral Metrics.}
further introduce:
\begin{itemize}
    \item \textit{Price Sensitivity Curves:} Binned regression and elasticity curves following~\cite{Krasnova2018}.
    \item \textit{Exploration–Exploitation Balance:} Cumulative unique SKU curves~\cite{Cui2018} and session based switching.
    \item \textit{Funnel Analysis:} End-to-end metrics for transitions from search $\rightarrow$ visit $\rightarrow$ cart $\rightarrow$ purchase, inspired by~\cite{Covington2016}.
    \item \textit{Session Progression Patterns:} Event distribution differences between session start and end, using techniques from~\cite{Quadrana2017}.
    \item \textit{Behavioral Segmentation:} Binary indicators for archetypes (e.g., loyalist, researcher, decisive), as suggested in~\cite{Christoffel2022}.
\end{itemize}

All features are aggregated and normalized on a per user basis. This comprehensive pipeline, grounded in both classical and recent research, ensures the autoencoder receives the most informative and transferable input for universal representation learning in large scale recommender systems.


\subsection{Model Architecture Overview}
\begin{figure}[t]
  \centering
  % Replace with actual diagram or export from Figma/draw.io as PDF/PNG for submission
  \includegraphics[width=0.9\linewidth]{high.png}
  \caption{Overview of the universal user profile generation pipeline. Engineered user features are transformed via a deep residual autoencoder with contrastive and noise based regularization to yield universal embeddings.}
  \label{fig:architecture}
\end{figure}

As shown in Figure~\ref{fig:architecture}, the pipeline begins by processing raw behavioral logs into a comprehensive set of engineered features. These features encompass temporal cycles, session statistics, conversion rates, and advanced behavioral metrics form a high dimensional input vector for each user.

At the heart of the system is a deep residual autoencoder, designed explicitly to address common challenges in high dimensional behavioral data. This autoencoder employs residual connections to facilitate gradient flow and prevent information loss~\cite{He2016ResNet,Mao2016}. The model incorporates curriculum noise injection, gradually annealing from higher to lower noise levels~\cite{Bengio2009Curriculum,Sajjadi2016NoiseAnneal}. Contrastive regularization (InfoNCE-style) promotes semantic differentiation in latent embeddings~\cite{Oord2018,Zhou2020S3Rec}.

Curriculum noise injection, gradually annealing from higher to lower noise levels, ensures that embeddings are robust and generalizable, effectively capturing both coarse grained and fine grained behavioral dynamics.

Contrastive regularization (InfoNCE-style)~\cite{Oord2018} actively promotes semantic differentiation in latent embeddings, enabling the model to distinctly represent user segments and individual behavioral patterns.

\textbf{Encoder:}  
The encoder comprises a stack of four fully connected layers with dimensions [512, 256, 128, 64], each followed by batch normalization and GELU activation. Residual skip connections are added after each layer to facilitate gradient flow and retain both high level and fine grained information from the input features. Essentially The four‐layer encoder acts as a progressive filter, distilling the hundreds of engineered signals into increasingly abstract representations.
Early layers (512 → 256) emphasize fine‑grained cues—recency of actions, session burstiness, price sensitivity—that are highly discriminative for churn (users at risk often show abrupt drops in activity momentum).Intermediate layers (256 → 128) begin to aggregate category‑level co‑occurrences (e.g., shifts from electronics to apparel), which drive the propensity category task.The deepest layer (128 → 64) captures cross‑SKU transition patterns and long‑tail exploration signals, essential for propensity SKU where subtle item–item relationships matter.
Residual skips ensure that all three tasks still “see” both low‑level recency signals (vital for churn) and high‑level co‑purchase signals (vital for propensity) without vanishing gradients.

\textbf{Latent Layer:}  
The bottleneck layer (dimension 32 or 64, depending on hyperparameter sweep) serves as the compact universal behavioral profile for each user. This latent representation is optimized not only for input reconstruction but also for generalization across downstream tasks.

\textbf{Decoder:}  
The decoder mirrors the encoder structure, expanding the latent vector back to the original feature dimension. This reconstruction objective ensures the embedding retains as much salient information from the original behavioral data as possible.

\textbf{Loss Functions:}  
The training objective combines three terms: (1) mean squared error (MSE) for feature reconstruction, (2) a lightweight InfoNCE contrastive loss computed between latent vectors of augmented (noisy) user feature pairs, and (3) L2 regularization on the latent embeddings. The total loss is  
\[
\mathcal{L}_{total} = \mathcal{L}_{rec} + \lambda_{con}\mathcal{L}_{con} + \lambda_{L2}\|\mathbf{z}\|_2^2
\]  
where $\lambda_{con}$ and $\lambda_{L2}$ are weighting coefficients determined via grid search.

\textbf{Curriculum Noise:}  
To improve robustness, Gaussian noise is added to the input features during training, with the noise variance annealed linearly from $\sigma=0.10$ to $\sigma=0.02$ over epochs. This encourages the autoencoder to learn stable, generalizable patterns before focusing on subtle, task specific details.

\begin{figure}[htb!]
  \centering
  % Replace with actual diagram or export from Figma/draw.io as PDF/PNG for submission
  \includegraphics[width=0.9\linewidth]{detail.png}
  \caption{Enhanced pipeline for generating universal user embeddings. Raw event logs are transformed through feature engineering, encoded via a residual autoencoder with curriculum noise and contrastive regularization, and finally distilled into universal embeddings usable across multiple predictive tasks.}
  \label{fig:architecture}
\end{figure}

\textbf{Integration with Feature Engineering:}  
The success of the model is closely linked to the expressiveness of the engineered features. High level constructs such as session progression, price sensitivity, and behavioral segmentation—not only expand the representational capacity of the latent embeddings but also promote transferability across varied prediction targets (e.g., churn, propensity, category preference). The autoencoder thus serves as a powerful information compressor, distilling hundreds of heterogeneous signals into a unified, compact profile for each user.

By combining advanced feature engineering with a robust, regularized autoencoder architecture, the system produces user embeddings that meet the RecSys Challenge 2025’s goals of universal, high utility representation.

\section{Results and Discussion}
The pipeline began with a comprehensive feature engineering process, transforming raw behavioral logs into rich user level vectors using temporal, session based, and advanced behavioral features~\cite{Quadrana2018, Christoffel2022}. These features enabled effective benchmarking of three fundamentally different user representation approaches: matrix factorization (SVD), sequential neural modeling (SASRec), and deep autoencoders.

\textbf{SVD Baseline.}
As a classical baseline, I trained a truncated singular value decomposition (SVD) model on the user–item interaction matrix. The resulting embeddings achieved competitive performance across all open tasks (AUROC: churn = 0.7301, category propensity = 0.7953, SKU propensity = 0.7854) and provided strong hidden task scores as well (hidden1 = 0.7612, hidden2 = 0.7777, hidden3 = 0.7921). This confirms the value of global, low rank structure in user behavior data~\cite{Koren2009}.

\textbf{SASRec.}
To capture sequential and temporal dependencies,  next step implemented a SASRec based model~\cite{Kang2018}. While SASRec slightly improved the open churn prediction score (0.7313 vs. 0.7301), its propensity metrics were marginally lower than SVD (category: 0.7872, SKU: 0.7821). Hidden task scores showed a small decrease (e.g., hidden1 = 0.7403), suggesting that pure sequence modeling alone was insufficient to generalize across all predictive tasks in this context.

\textbf{Autoencoder (Final Submission).}
The final approach employed a deep autoencoder with residual connections, contrastive loss, and curriculum noise. This model leveraged the full breadth of engineered features and demonstrated consistent improvements on both open and hidden tasks: churn (0.7325), category propensity (0.7908), SKU propensity (0.7836), and notably robust hidden task scores (e.g., hidden3 = 0.795). The autoencoder’s ability to compress heterogeneous, high dimensional signals into compact, information rich representations proved key towards outperforming both SVD and SASRec.

\textbf{Comparative Insights.}
These results highlight that while traditional matrix factorization (SVD) captures static structure well, and transformer based models (SASRec) excel in modeling sequences, neither alone sufficed for the challenge’s universal profiling requirements. Only by combining extensive feature engineering with modern unsupervised deep learning (autoencoders) did I achieve the best trade off between interpretability, transferability, and performance across disclosed and undisclosed tasks.

The findings demonstrate the critical importance of rich feature engineering and robust representation learning methods in large scale user modeling. Deep autoencoders, when paired with advanced behavioral features, offer a practical and effective solution for universal user profiling in multi task recommender system challenges like RecSys 2025.
\label{sec:experiments}
\begin{table}[htb!]
\centering
	\resizebox{\linewidth}{!}{%
  	\begin{tabular}{lcccccc}
    	\toprule
    	\textbf{Method} & \textbf{Churn} & \textbf{Prop. Cat.} & \textbf{Prop. SKU} & \textbf{Hidden1} & \textbf{Hidden2} & \textbf{Hidden3} \\
    	\midrule
    	SVD         & 0.7301 & 0.7953 & 0.7854 & 0.7612 & 0.7777 & 0.7921 \\
    	SASRec      & 0.7313 & 0.7872 & 0.7821 & 0.7403 & 0.7817 & 0.7917 \\
    	Autoencoder & 0.7325 & 0.7908 & 0.7836 & 0.7434 & 0.7816 & 0.7950 \\
    	\bottomrule
  \end{tabular}
	}
	\caption{Performance comparison (AUROC) across open and hidden tasks.}
	\label{tab:results}
	\end{table}

\section{Conclusion}
This paper presents a comprehensive approach to universal user embedding generation in the context of the RecSys Challenge 2025. The proposed methodology combines extensive behavioral feature engineering with an advanced deep residual autoencoder architecture, enhanced through contrastive learning and curriculum noise regularization. Through rigorous experimental evaluation, I demonstrated that this integrated approach successfully addresses the challenge's core objective: creating compact, generalizable user representations that perform consistently across diverse predictive tasks including churn prediction, category propensity, and SKU propensity.

The comparative analysis revealed significant advantages over established baseline methods. While SVD effectively captured static patterns and SASRec excelled at modeling sequential dependencies, neither approach alone sufficed for truly universal representations. The proposed autoencoder framework, by leveraging both rich behavioral features and sophisticated regularization techniques, achieved superior performance across all evaluation metrics. The residual connections facilitated efficient gradient flow through deep architectures, while contrastive regularization and curriculum-based noise injection substantially enhanced embedding quality and cross-task generalization capability.

Future work could extend this approach in several promising directions: incorporating additional unsupervised learning strategies, exploring hierarchical or temporally-aware contrastive objectives, and applying these embeddings to more complex real-time prediction scenarios. These advancements would further improve user modeling performance and generalization capabilities across increasingly diverse recommendation contexts, potentially bridging the gap between academic research and practical industry applications.
\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
